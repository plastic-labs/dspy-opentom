{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy + OpenTom\n",
    "\n",
    "Goal of this notebook is to explore the OpenToM dataset and see if we can write some DSPy code to optimize prompts for answering the questions.\n",
    "\n",
    "They've evaluated the performance of CoT and SimToM on their dataset, I now wonder how much extra performance we can get from using a framework like DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP: run poetry install + shell in the terminal, then i just say `cursor .` to open my editor and it runs this nb in the venv\n",
    "# GETTING STARTED: let's import the packages and get the data\n",
    "import dspy\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # need ur api keys set beforehand\n",
    "\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=200)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "\n",
    "# dataset isn't able to be loaded using hf datasets package so let's read it from github raw\n",
    "# also let's keep it simple and just go for the opentom_long.json\n",
    "# this is the one that they sampled 100 existing OpenToM plots to produce \"extra long\" narratives\n",
    "# url = \"https://raw.githubusercontent.com/SeacowX/OpenToM/main/data/opentom_long.json\"\n",
    "url = \"https://raw.githubusercontent.com/SeacowX/OpenToM/main/data/opentom.json\"\n",
    "response = requests.get(url).json()\n",
    "\n",
    "df = pd.DataFrame(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]['plot_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_counts = df['question'].apply(lambda x: x['type']).value_counts()\n",
    "type_counts  # fo means first-order, so means second-order\n",
    "\n",
    "# first order questions  directly ask about a character’s perception of the world, while\n",
    "# second order questions ask about a character’s belief of another character's mental state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and it contains a 'question' column with dictionaries having 'type' and 'answer' keys\n",
    "\n",
    "# Extract 'type' and 'answer' into separate columns\n",
    "df['type'] = df['question'].apply(lambda x: x['type'])\n",
    "df['answer'] = df['question'].apply(lambda x: x['answer'])\n",
    "\n",
    "# Group by 'type' and get unique 'answer' values for each 'type'\n",
    "unique_answers_by_type = df.groupby('type')['answer'].unique()\n",
    "\n",
    "print(unique_answers_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# convert the dataset to what DSPy expects (list of Example objects)\n",
    "dataset = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    context = row['narrative']\n",
    "    question = row['question']['question']\n",
    "    answer = row['question']['answer']\n",
    "    type = row['question']['type']\n",
    "    plot_info = json.dumps(row['plot_info']) # Keeping each example field as a string might be a good idea\n",
    "\n",
    "    if \"location\" in type and (answer.lower().strip() != \"yes\" and answer.lower().strip() != \"no\"): # don't provide answer choices for fine grained location questions\n",
    "        answer_choices = \"n/a, list a specific location\"\n",
    "    elif \"location\" in type:\n",
    "        answer_choices = \"No, Yes\"\n",
    "    else:\n",
    "        answer_choices = \", \".join(unique_answers_by_type[type])\n",
    "\n",
    "    dataset.append(dspy.Example(context=context, question=question, answer=answer, type=type, plot_info=plot_info, answer_choices=answer_choices).with_inputs(\"context\", \"question\", \"answer_choices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets by question types \n",
    "from collections import defaultdict\n",
    "\n",
    "datasets = defaultdict(lambda: [])\n",
    "\n",
    "for example in dataset:\n",
    "    datasets[example.type].append(example)\n",
    "\n",
    "datasets.keys()\n",
    "[len(dataset) for dataset in datasets.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test split\n",
    "for question_type, dataset in datasets.items():\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    datasets[question_type] = {\n",
    "        \"train\": dataset[:int(len(dataset) * 0.8)],\n",
    "        \"test\": dataset[int(len(dataset) * 0.8):],\n",
    "    }\n",
    "\n",
    "    print(f\"Now Train {question_type}: {len(datasets[question_type]['train'])}\")\n",
    "    print(f\"Now Test {question_type}: {len(datasets[question_type]['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Signatures\n",
    "\n",
    "Using a \"Baleen\" pipeline [(Khattab et al., 2021)](https://arxiv.org/abs/2101.00436)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer the question\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Generate answers to the questions\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts and psychological insights\")\n",
    "    question = dspy.InputField()\n",
    "    answer_choices = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "# generate a question to help you better answer the question\n",
    "# class GenerateSearchQuery(dspy.Signature):\n",
    "#     \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "#     context = dspy.InputField(desc=\"may contain relevant facts and psychological insights\")\n",
    "#     question = dspy.InputField()\n",
    "#     query = dspy.OutputField(desc=\"a thought that might help answer the question\") \n",
    "\n",
    "# class GenerateSearchAnswer(dspy.Signature):\n",
    "#     \"\"\"Generate a long form answer to the question given the context\"\"\"\n",
    "\n",
    "#     context = dspy.InputField(desc=\"may contain relevant facts and psychological insights\")\n",
    "#     question = dspy.InputField()\n",
    "#     answer = dspy.OutputField(desc=\"a thought about what the answer to the question may be\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate\n",
    "\n",
    "class SimplifiedBaleen(dspy.Module):\n",
    "    # def __init__(self, max_hops=2):\n",
    "    #     super().__init__()\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
    "        # self.generate_search_answer = dspy.ChainOfThought(GenerateSearchAnswer)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        # self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question, context, answer_choices):\n",
    "        # final_context = []\n",
    "        \n",
    "        # for hop in range(self.max_hops):\n",
    "        #     query = self.generate_query[hop](context=context, question=question).query\n",
    "        #     filtered_context = self.generate_search_answer(context=context, question=query).answer\n",
    "        #     final_context = (context + filtered_context)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question, answer_choices=answer_choices)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're defining a simple signature just to generate the answer given the context, question, and answer choices.\n",
    "\n",
    "# Executing the Pipeline\n",
    "\n",
    "Let's see how this works in a zero-shot setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = datasets[\"attitude\"][\"test\"][0].question\n",
    "my_context = datasets[\"attitude\"][\"test\"][0].context\n",
    "my_answer_choices = datasets[\"attitude\"][\"test\"][0].answer_choices\n",
    "\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\n",
    "pred = uncompiled_baleen(question=my_question, context=my_context, answer_choices=my_answer_choices)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"True Answer: {datasets['attitude']['test'][0].answer}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Answer Choices: {my_answer_choices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentom_evaluator import OpenToMEvaluatorDspy\n",
    "eval = OpenToMEvaluatorDspy()\n",
    "eval.dspy_metric(datasets[\"attitude\"][\"test\"][0], pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the last three calls to the LM (i.e., generating the first hop's query, generating the second hop's query, and generating the answer) using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the Pipeline\n",
    "\n",
    "However, a zero-shot approach quickly falls short for more specialized tasks, novel domains/settings, and more efficient (or open) models.\n",
    "\n",
    "To address this, DSPy offers compilation. Let's compile our multi-hop (SimplifiedBaleen) program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentom_evaluator import OpenToMEvaluatorDspy\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "import time\n",
    "\n",
    "eval_question_types = [\"attitude\", \"multihop-fo\", \"multihop-so\", \"location-fo\", \"location-so\"] # question types to optimize a module for\n",
    "modules = {}\n",
    "\n",
    "# define modules for each question type\n",
    "for question_type in eval_question_types:\n",
    "    print(f\"TYPE: {question_type}\")\n",
    "    evaluator = OpenToMEvaluatorDspy(model_name=\"(training set) complied baleen\")\n",
    "    optimizer = BootstrapFewShotWithRandomSearch(metric=evaluator.dspy_metric, num_threads=1)\n",
    "    compiled_baleen = optimizer.compile(SimplifiedBaleen(), trainset=datasets[question_type][\"train\"][:25])\n",
    "\n",
    "    modules[question_type] = compiled_baleen\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "print(\"Macro Averaged F1 Scores\")\n",
    "for question_type in eval_question_types:\n",
    "    test = datasets[question_type][\"test\"]\n",
    "    compiled_baleen = modules[question_type]\n",
    "\n",
    "    # Set up the `evaluate_on_hotpotqa` function.\n",
    "    evaluate_on_opentom = Evaluate(devset=test[:10], num_threads=1, display_progress=True, display_table=10)\n",
    "\n",
    "    uncompiled_baleen_evaluator = OpenToMEvaluatorDspy(model_name='uncompiled_baleen')\n",
    "    uncompiled_baleen_retrieval_score = evaluate_on_opentom(uncompiled_baleen, metric=uncompiled_baleen_evaluator.dspy_metric, display=False)\n",
    "    uncompiled_baleen_evaluator.print_f1_results()\n",
    "\n",
    "    compiled_baleen_evaluator = OpenToMEvaluatorDspy(model_name='compiled_baleen')\n",
    "    compiled_baleen_retrieval_score = evaluate_on_opentom(compiled_baleen, metric=compiled_baleen_evaluator.dspy_metric, display=False)\n",
    "    compiled_baleen_evaluator.print_f1_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_baleen.dump_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_baleen.dump_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
